from huggingface_hub import hf_hub_download
import joblib
import tldextract
import numpy as np
from scipy.sparse import hstack
from sklearn.feature_extraction.text import HashingVectorizer

_model_path = hf_hub_download(
    repo_id="asobirov/dga-detector",
    filename="dga_detector_full.joblib"
)
_clf = joblib.load(_model_path)

_vectorizer = HashingVectorizer(
    analyzer="char",
    ngram_range=(2,5),
    n_features=2**20,
    alternate_sign=False
)

# Helper functions for preprocessing
def _extract_base(domains):
    out = []
    for d in domains:
        ext = tldextract.extract(d.lower())
        out.append(f"{ext.domain}{ext.suffix}")
    return out

def _compute_stats(domains):
    feats = []
    for d in domains:
        s = d.lower()
        L = len(s)
        digit_ratio = sum(c.isdigit() for c in s) / max(L, 1)
        ps = [s.count(c) / L for c in set(s)]
        entropy = -sum(p * np.log2(p) for p in ps)
        feats.append([L, digit_ratio, entropy])
    return np.array(feats)

def eval_gda(domain: str) -> float:
    """
    Returns the probability that `domain` is generated by a DGA.
    """
    base = _extract_base([domain])
    X_text  = _vectorizer.transform(base)
    X_stats = _compute_stats(base)
    X_all   = hstack([X_text, X_stats])
    return float(_clf.predict_proba(X_all)[0, 1])